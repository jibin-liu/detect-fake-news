{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 3)\n",
      "                                                   title  \\\n",
      "8476                        You Can Smell Hillary’s Fear   \n",
      "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "3608         Kerry to go to Paris in gesture of sympathy   \n",
      "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "875     The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                    text label  \n",
      "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "875    It's primary day in New York and front-runners...  REAL  \n"
     ]
    }
   ],
   "source": [
    "# import libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load data into dataframe\n",
    "data_path = './data/fake_or_real_news.csv'\n",
    "df = pd.read_csv(data_path, index_col=0)\n",
    "\n",
    "# get data shape and preview\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "FAKE    3164\n",
       "REAL    3171\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of FAKE and REAL rows\n",
    "df.groupby(['label'])['label'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1502    Republican presidential frontrunner Donald Tru...\n",
       "3072    Data scientists at Facebook recently published...\n",
       "8996    Get short URL 0 0 0 0 US Deputy Secretary of D...\n",
       "799     Republican presidential candidate Donald Trump...\n",
       "976     Republican presidential candidate Ted Cruz is ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split features and labels\n",
    "labels = df['label']\n",
    "features = df['text']\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=25)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import classes\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4751, 59469)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create count_vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# fit and transform X_train\n",
    "X_train_count_vec = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# transfor X_test\n",
    "X_test_count_vec = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ťthird',\n",
       " 'ťtwo',\n",
       " 'ťwho',\n",
       " 'ťđ',\n",
       " 'ź50',\n",
       " 'νοεμβρίου',\n",
       " 'главная',\n",
       " 'октября',\n",
       " 'яркий',\n",
       " 'القادمون']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create tfidf vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# fit and transform X_train\n",
    "X_train_tfidf_vec = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# transform X_test\n",
    "X_test_tfidf_vec = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '000000031',\n",
       " '00000031',\n",
       " '000035',\n",
       " '00006',\n",
       " '0001',\n",
       " '0001pt',\n",
       " '0002']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the bag of words, part of them are non-sense words, such as '00' and non-English characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit and transform Y_train\n",
    "y_train_encode = le.fit_transform(y_train)\n",
    "\n",
    "# transform Y_test\n",
    "y_test_encode = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAKE', 'REAL'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using naive bayes model (MultinormialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Count Vectorizer method is 0.9002525252525253\n",
      "The accuracy of TF-IDF Vectorizer method is 0.8535353535353535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def predict(X_train, X_test, vectorizer):\n",
    "    classifer = MultinomialNB()\n",
    "    classifer.fit(X_train, y_train_encode)\n",
    "    y_predict = classifer.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test_encode, y_predict)\n",
    "\n",
    "    print('The accuracy of {} Vectorizer method is {}'.format(vectorizer, accuracy))\n",
    "\n",
    "\n",
    "predict(X_train_count_vec, X_test_count_vec, 'Count')\n",
    "predict(X_train_tfidf_vec, X_test_tfidf_vec, 'TF-IDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra - using Pipeline and GridSearchCV to find better parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Grid search CV is 0.9191919191919192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "pipe = Pipeline([('vector', CountVectorizer()),\n",
    "                 ('clf', MultinomialNB())])\n",
    "\n",
    "# specify parameter options and distributions to try\n",
    "# max_df = [0.3, 0.5, 0.7, 1]\n",
    "# alpha = [0.3, 0.5, 0.7, 1]\n",
    "\n",
    "# count_vectors = [CountVectorizer(max_df=md) for md in max_df]\n",
    "# tfidf_vectors = [TfidfVectorizer(max_df=md) for md in max_df]\n",
    "\n",
    "# cls = [MultinomialNB(alpha=a) for a in alpha]\n",
    "\n",
    "param_dist = {\n",
    "    'vector': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vector__max_df': uniform.rvs(size=1000, random_state=25),\n",
    "    'vector__lowercase': [True, False],\n",
    "    'vector__stop_words': ['english', None],\n",
    "    'clf__alpha': uniform.rvs(size=1000, random_state=25)\n",
    "}\n",
    "\n",
    "# create GridSearchCV\n",
    "grid = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=10)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# make prediction\n",
    "test_accuracy = grid.score(X_test, y_test)\n",
    "print('Accuracy of Grid search CV is {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_vector</th>\n",
       "      <th>param_vector__lowercase</th>\n",
       "      <th>param_vector__max_df</th>\n",
       "      <th>param_vector__stop_words</th>\n",
       "      <th>params</th>\n",
       "      <th>...</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_test_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_test_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.149603</td>\n",
       "      <td>0.243885</td>\n",
       "      <td>0.890549</td>\n",
       "      <td>0.949601</td>\n",
       "      <td>0.588797</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.375215</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vector__stop_words': None, 'vector__max_df':...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.949953</td>\n",
       "      <td>0.892632</td>\n",
       "      <td>0.949252</td>\n",
       "      <td>0.888186</td>\n",
       "      <td>0.949264</td>\n",
       "      <td>0.076742</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>0.011149</td>\n",
       "      <td>0.001351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.115995</td>\n",
       "      <td>0.219956</td>\n",
       "      <td>0.909703</td>\n",
       "      <td>0.962745</td>\n",
       "      <td>0.661949</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.212457</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vector__stop_words': 'english', 'vector__max...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924211</td>\n",
       "      <td>0.960711</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.962816</td>\n",
       "      <td>0.902954</td>\n",
       "      <td>0.962824</td>\n",
       "      <td>0.030318</td>\n",
       "      <td>0.006661</td>\n",
       "      <td>0.010293</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.116126</td>\n",
       "      <td>0.220981</td>\n",
       "      <td>0.913702</td>\n",
       "      <td>0.966136</td>\n",
       "      <td>0.433797</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.719055</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vector__stop_words': 'english', 'vector__max...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924211</td>\n",
       "      <td>0.964453</td>\n",
       "      <td>0.917895</td>\n",
       "      <td>0.965622</td>\n",
       "      <td>0.907173</td>\n",
       "      <td>0.964695</td>\n",
       "      <td>0.029326</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>0.007576</td>\n",
       "      <td>0.001099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.121314</td>\n",
       "      <td>0.223713</td>\n",
       "      <td>0.864660</td>\n",
       "      <td>0.929629</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.665257</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vector__stop_words': 'english', 'vector__max...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869474</td>\n",
       "      <td>0.928438</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.931244</td>\n",
       "      <td>0.869198</td>\n",
       "      <td>0.927285</td>\n",
       "      <td>0.032162</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.011409</td>\n",
       "      <td>0.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.226498</td>\n",
       "      <td>0.249053</td>\n",
       "      <td>0.911177</td>\n",
       "      <td>0.961903</td>\n",
       "      <td>0.505411</td>\n",
       "      <td>CountVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.686236</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vector__stop_words': None, 'vector__max_df':...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.928421</td>\n",
       "      <td>0.960711</td>\n",
       "      <td>0.913684</td>\n",
       "      <td>0.962348</td>\n",
       "      <td>0.909283</td>\n",
       "      <td>0.961188</td>\n",
       "      <td>0.023214</td>\n",
       "      <td>0.006201</td>\n",
       "      <td>0.008931</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.138186</td>\n",
       "      <td>0.225069</td>\n",
       "      <td>0.849926</td>\n",
       "      <td>0.914498</td>\n",
       "      <td>0.80522</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.921901</td>\n",
       "      <td>english</td>\n",
       "      <td>{'vector__stop_words': 'english', 'vector__max...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.915809</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.915341</td>\n",
       "      <td>0.856540</td>\n",
       "      <td>0.910919</td>\n",
       "      <td>0.035922</td>\n",
       "      <td>0.008178</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.002136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.233769</td>\n",
       "      <td>0.247308</td>\n",
       "      <td>0.863608</td>\n",
       "      <td>0.930868</td>\n",
       "      <td>0.437611</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.910929</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vector__stop_words': None, 'vector__max_df':...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865263</td>\n",
       "      <td>0.931244</td>\n",
       "      <td>0.858947</td>\n",
       "      <td>0.931712</td>\n",
       "      <td>0.871308</td>\n",
       "      <td>0.927519</td>\n",
       "      <td>0.026698</td>\n",
       "      <td>0.005981</td>\n",
       "      <td>0.008164</td>\n",
       "      <td>0.001338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.221219</td>\n",
       "      <td>0.248080</td>\n",
       "      <td>0.870133</td>\n",
       "      <td>0.935452</td>\n",
       "      <td>0.514244</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.325647</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vector__stop_words': None, 'vector__max_df':...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.934284</td>\n",
       "      <td>0.871579</td>\n",
       "      <td>0.936857</td>\n",
       "      <td>0.871308</td>\n",
       "      <td>0.933131</td>\n",
       "      <td>0.017949</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>0.001281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.230145</td>\n",
       "      <td>0.254175</td>\n",
       "      <td>0.866765</td>\n",
       "      <td>0.932529</td>\n",
       "      <td>0.519465</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.374185</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vector__stop_words': None, 'vector__max_df':...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867368</td>\n",
       "      <td>0.933583</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.934518</td>\n",
       "      <td>0.873418</td>\n",
       "      <td>0.930325</td>\n",
       "      <td>0.022595</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.001278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.219765</td>\n",
       "      <td>0.239571</td>\n",
       "      <td>0.879394</td>\n",
       "      <td>0.971819</td>\n",
       "      <td>0.408203</td>\n",
       "      <td>TfidfVectorizer(analyzer='word', binary=False,...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0314957</td>\n",
       "      <td>None</td>\n",
       "      <td>{'vector__stop_words': None, 'vector__max_df':...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877895</td>\n",
       "      <td>0.972404</td>\n",
       "      <td>0.877895</td>\n",
       "      <td>0.971235</td>\n",
       "      <td>0.896624</td>\n",
       "      <td>0.971475</td>\n",
       "      <td>0.027118</td>\n",
       "      <td>0.006756</td>\n",
       "      <td>0.012197</td>\n",
       "      <td>0.001009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "0       2.149603         0.243885         0.890549          0.949601   \n",
       "1       2.115995         0.219956         0.909703          0.962745   \n",
       "2       2.116126         0.220981         0.913702          0.966136   \n",
       "3       2.121314         0.223713         0.864660          0.929629   \n",
       "4       2.226498         0.249053         0.911177          0.961903   \n",
       "5       2.138186         0.225069         0.849926          0.914498   \n",
       "6       2.233769         0.247308         0.863608          0.930868   \n",
       "7       2.221219         0.248080         0.870133          0.935452   \n",
       "8       2.230145         0.254175         0.866765          0.932529   \n",
       "9       2.219765         0.239571         0.879394          0.971819   \n",
       "\n",
       "  param_clf__alpha                                       param_vector  \\\n",
       "0         0.588797  CountVectorizer(analyzer='word', binary=False,...   \n",
       "1         0.661949  CountVectorizer(analyzer='word', binary=False,...   \n",
       "2         0.433797  CountVectorizer(analyzer='word', binary=False,...   \n",
       "3         0.591205  TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "4         0.505411  CountVectorizer(analyzer='word', binary=False,...   \n",
       "5          0.80522  TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "6         0.437611  TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "7         0.514244  TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "8         0.519465  TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "9         0.408203  TfidfVectorizer(analyzer='word', binary=False,...   \n",
       "\n",
       "  param_vector__lowercase param_vector__max_df param_vector__stop_words  \\\n",
       "0                    True             0.375215                     None   \n",
       "1                   False             0.212457                  english   \n",
       "2                   False             0.719055                  english   \n",
       "3                   False             0.665257                  english   \n",
       "4                   False             0.686236                     None   \n",
       "5                   False             0.921901                  english   \n",
       "6                   False             0.910929                     None   \n",
       "7                   False             0.325647                     None   \n",
       "8                    True             0.374185                     None   \n",
       "9                    True            0.0314957                     None   \n",
       "\n",
       "                                              params       ...         \\\n",
       "0  {'vector__stop_words': None, 'vector__max_df':...       ...          \n",
       "1  {'vector__stop_words': 'english', 'vector__max...       ...          \n",
       "2  {'vector__stop_words': 'english', 'vector__max...       ...          \n",
       "3  {'vector__stop_words': 'english', 'vector__max...       ...          \n",
       "4  {'vector__stop_words': None, 'vector__max_df':...       ...          \n",
       "5  {'vector__stop_words': 'english', 'vector__max...       ...          \n",
       "6  {'vector__stop_words': None, 'vector__max_df':...       ...          \n",
       "7  {'vector__stop_words': None, 'vector__max_df':...       ...          \n",
       "8  {'vector__stop_words': None, 'vector__max_df':...       ...          \n",
       "9  {'vector__stop_words': None, 'vector__max_df':...       ...          \n",
       "\n",
       "   split7_test_score  split7_train_score  split8_test_score  \\\n",
       "0           0.905263            0.949953           0.892632   \n",
       "1           0.924211            0.960711           0.905263   \n",
       "2           0.924211            0.964453           0.917895   \n",
       "3           0.869474            0.928438           0.863158   \n",
       "4           0.928421            0.960711           0.913684   \n",
       "5           0.852632            0.915809           0.852632   \n",
       "6           0.865263            0.931244           0.858947   \n",
       "7           0.873684            0.934284           0.871579   \n",
       "8           0.867368            0.933583           0.873684   \n",
       "9           0.877895            0.972404           0.877895   \n",
       "\n",
       "   split8_train_score  split9_test_score  split9_train_score  std_fit_time  \\\n",
       "0            0.949252           0.888186            0.949264      0.076742   \n",
       "1            0.962816           0.902954            0.962824      0.030318   \n",
       "2            0.965622           0.907173            0.964695      0.029326   \n",
       "3            0.931244           0.869198            0.927285      0.032162   \n",
       "4            0.962348           0.909283            0.961188      0.023214   \n",
       "5            0.915341           0.856540            0.910919      0.035922   \n",
       "6            0.931712           0.871308            0.927519      0.026698   \n",
       "7            0.936857           0.871308            0.933131      0.017949   \n",
       "8            0.934518           0.873418            0.930325      0.022595   \n",
       "9            0.971235           0.896624            0.971475      0.027118   \n",
       "\n",
       "   std_score_time  std_test_score  std_train_score  \n",
       "0        0.009199        0.011149         0.001351  \n",
       "1        0.006661        0.010293         0.001200  \n",
       "2        0.006829        0.007576         0.001099  \n",
       "3        0.005072        0.011409         0.001403  \n",
       "4        0.006201        0.008931         0.001244  \n",
       "5        0.008178        0.014400         0.002136  \n",
       "6        0.005981        0.008164         0.001338  \n",
       "7        0.007690        0.006734         0.001281  \n",
       "8        0.006164        0.011657         0.001278  \n",
       "9        0.006756        0.012197         0.001009  \n",
       "\n",
       "[10 rows x 35 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
